<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>Load file to staging</name>
    <description/>
    <extended_description/>
    <job_version/>
    <job_status>0</job_status>
  <directory>&#47;</directory>
  <created_user>-</created_user>
  <created_date>2013&#47;11&#47;25 16:28:48.356</created_date>
  <modified_user>-</modified_user>
  <modified_date>2013&#47;11&#47;25 16:28:48.356</modified_date>
    <parameters>
        <parameter>
            <name>local_root_dir_content_logs</name>
            <default_value/>
            <description/>
        </parameter>
        <parameter>
            <name>log_file_partent</name>
            <default_value/>
            <description/>
        </parameter>
        <parameter>
            <name>log_file_sql_partent</name>
            <default_value/>
            <description/>
        </parameter>
        <parameter>
            <name>report_end_date</name>
            <default_value/>
            <description/>
        </parameter>
        <parameter>
            <name>report_start_date</name>
            <default_value/>
            <description/>
        </parameter>
    </parameters>
  <connection>
    <name>Hadoop ecep</name>
    <server>ecep1</server>
    <type>HIVE</type>
    <access>Native</access>
    <database>default</database>
    <port>10000</port>
    <username/>
    <password>Encrypted </password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>10000</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>N</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>IBconnection</name>
    <server>ec2-107-22-90-50.compute-1.amazonaws.com</server>
    <type>MYSQL</type>
    <access>Native</access>
    <database>binh</database>
    <port>5029</port>
    <username>nhut</username>
    <password>Encrypted 2be98afc86aa7f2e4cb79807c8ec0fe82</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>EXTRA_OPTION_MYSQL.defaultFetchSize</code><attribute>500</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.useCursorFetch</code><attribute>true</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>5029</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>STREAM_RESULTS</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>N</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
  <connection>
    <name>Postgres</name>
    <server>localhost</server>
    <type>POSTGRESQL</type>
    <access>JNDI</access>
    <database>LocalPostgres</database>
    <port>5432</port>
    <username/>
    <password>Encrypted </password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute><code>FORCE_IDENTIFIERS_TO_LOWERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>FORCE_IDENTIFIERS_TO_UPPERCASE</code><attribute>N</attribute></attribute>
      <attribute><code>IS_CLUSTERED</code><attribute>N</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>5432</attribute></attribute>
      <attribute><code>QUOTE_ALL_FIELDS</code><attribute>N</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>N</attribute></attribute>
      <attribute><code>USE_POOLING</code><attribute>N</attribute></attribute>
    </attributes>
  </connection>
    <slaveservers>
    </slaveservers>
<job-log-table><connection/>
<schema/>
<table/>
<size_limit_lines/>
<interval/>
<timeout_days/>
<field><id>ID_JOB</id><enabled>Y</enabled><name>ID_JOB</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>JOBNAME</id><enabled>Y</enabled><name>JOBNAME</name></field><field><id>STATUS</id><enabled>Y</enabled><name>STATUS</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>STARTDATE</id><enabled>Y</enabled><name>STARTDATE</name></field><field><id>ENDDATE</id><enabled>Y</enabled><name>ENDDATE</name></field><field><id>LOGDATE</id><enabled>Y</enabled><name>LOGDATE</name></field><field><id>DEPDATE</id><enabled>Y</enabled><name>DEPDATE</name></field><field><id>REPLAYDATE</id><enabled>Y</enabled><name>REPLAYDATE</name></field><field><id>LOG_FIELD</id><enabled>Y</enabled><name>LOG_FIELD</name></field></job-log-table>
<jobentry-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>JOBNAME</id><enabled>Y</enabled><name>TRANSNAME</name></field><field><id>JOBENTRYNAME</id><enabled>Y</enabled><name>STEPNAME</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>RESULT</id><enabled>Y</enabled><name>RESULT</name></field><field><id>NR_RESULT_ROWS</id><enabled>Y</enabled><name>NR_RESULT_ROWS</name></field><field><id>NR_RESULT_FILES</id><enabled>Y</enabled><name>NR_RESULT_FILES</name></field><field><id>LOG_FIELD</id><enabled>N</enabled><name>LOG_FIELD</name></field><field><id>COPY_NR</id><enabled>N</enabled><name>COPY_NR</name></field></jobentry-log-table>
<channel-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>LOGGING_OBJECT_TYPE</id><enabled>Y</enabled><name>LOGGING_OBJECT_TYPE</name></field><field><id>OBJECT_NAME</id><enabled>Y</enabled><name>OBJECT_NAME</name></field><field><id>OBJECT_COPY</id><enabled>Y</enabled><name>OBJECT_COPY</name></field><field><id>REPOSITORY_DIRECTORY</id><enabled>Y</enabled><name>REPOSITORY_DIRECTORY</name></field><field><id>FILENAME</id><enabled>Y</enabled><name>FILENAME</name></field><field><id>OBJECT_ID</id><enabled>Y</enabled><name>OBJECT_ID</name></field><field><id>OBJECT_REVISION</id><enabled>Y</enabled><name>OBJECT_REVISION</name></field><field><id>PARENT_CHANNEL_ID</id><enabled>Y</enabled><name>PARENT_CHANNEL_ID</name></field><field><id>ROOT_CHANNEL_ID</id><enabled>Y</enabled><name>ROOT_CHANNEL_ID</name></field></channel-log-table>
   <pass_batchid>N</pass_batchid>
   <shared_objects_file/>
  <entries>
    <entry>
      <name>START</name>
      <description/>
      <type>SPECIAL</type>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>36</xloc>
      <yloc>75</yloc>
      </entry>
    <entry>
      <name>Create Working Tables</name>
      <description/>
      <type>SQL</type>
      <sql>add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;csv-serde-1.1.2.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-core-asl-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-jaxrs-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-mapper-asl-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-xc-1.8.8.jar;

DROP TABLE IF EXISTS daily_millenial_log;
CREATE EXTERNAL TABLE IF NOT EXISTS daily_millenial_log(
id BIGINT,
name STRING,
`date` STRING,
requests BIGINT,
ads_served BIGINT,
fill_rate_percentage DOUBLE,
clicks INT,
click_thru_rate_percentage DOUBLE,
net_revenue DOUBLE,
net_ecpm DOUBLE
)
row format serde &apos;com.bizo.hive.serde.csv.CSVSerde&apos;
STORED AS TEXTFILE
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;logs&#47;daily_millenial_log_${report_start_date}-${report_end_date}&apos;;

DROP TABLE IF EXISTS gen_millenial_performance;
CREATE EXTERNAL TABLE IF NOT EXISTS gen_millenial_performance(
gen_millenial_performance_id INT,
id STRING,
name STRING,
report_date STRING,
requests STRING,
ads_served STRING,
fill_rate_percentage STRING,
clicks STRING,
click_thru_rate_percentage STRING,
net_revenue STRING,
net_ecpm STRING,
data_file_id INT,
dt_lastchanged STRING,
eastern_date STRING,
eastern_time STRING,
local_date STRING,
local_time STRING,
gmt_date STRING,
gmt_time STRING,
response_date_pacific STRING,
eastern_date_sk INT,
eastern_time_sk INT,
local_date_sk INT,
local_time_sk INT,
gmt_date_sk INT,
gmt_time_sk INT,
partner_sk INT,
portal_sk INT,
partner_keyword STRING,
portal_keyword STRING
)
STORED AS TEXTFILE
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;staging&#47;gen_millenial_performance&#47;&apos;;

DROP TABLE IF EXISTS gen_millenial_performance_1;
CREATE TABLE gen_millenial_performance_1
LIKE gen_millenial_performance
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;staging&#47;gen_millenial_performance_1&#47;&apos;;

DROP TABLE IF EXISTS gen_millenial_performance_2;
CREATE TABLE gen_millenial_performance_2
LIKE gen_millenial_performance
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;staging&#47;gen_millenial_performance_2&#47;&apos;;

DROP TABLE IF EXISTS gen_millenial_performance_3;
CREATE TABLE gen_millenial_performance_3
LIKE gen_millenial_performance
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;staging&#47;gen_millenial_performance_3&#47;&apos;;

DROP TABLE IF EXISTS fact_mm_performance;
CREATE EXTERNAL TABLE IF NOT EXISTS fact_mm_performance(
  eastern_date_sk INT,
  eastern_time_sk INT,
  local_date_sk INT,
  local_time_sk INT,
  gmt_date_sk INT,
  gmt_time_sk INT,
  partner_sk INT,
  portal_sk INT,
  id INT,
  name STRING,
  requests INT,
  ads_served INT,
  fill_rate DOUBLE,
  clicks INT,
  click_thru_rate DOUBLE,
  net_revenue DOUBLE,
  net_ecpm DOUBLE,
  gen_adnetwork_performance_id INT,
  data_file_id INT
)
STORED AS TEXTFILE
LOCATION &apos;hdfs:&#47;&#47;ecep1:9000&#47;user&#47;hive&#47;warehouse&#47;staging&#47;fact_mm_performance&#47;&apos;;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>Hadoop ecep</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>594</xloc>
      <yloc>30</yloc>
      </entry>
    <entry>
      <name>Insert log data to staging</name>
      <description/>
      <type>SQL</type>
      <sql>add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;csv-serde-1.1.2.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-core-asl-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-jaxrs-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-mapper-asl-1.8.8.jar;
add jar &#47;usr&#47;local&#47;hive&#47;lib&#47;jackson-xc-1.8.8.jar;

INSERT OVERWRITE TABLE gen_millenial_performance 
SELECT 
unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;),
id,
name,
`date`,
requests,
ads_served,
fill_rate_percentage,
clicks,
click_thru_rate_percentage,
net_revenue,
net_ecpm,
-100,
unix_timestamp(), 
from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;yyyy-MM-dd&apos;) AS eastern_date, 
from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;HH:mm:ss&apos;) AS eastern_time, 
from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;yyyy-MM-dd&apos;) AS local_date,
from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;HH:mm:ss&apos;) AS local_time,
from_unixtime(unix_timestamp(from_utc_timestamp(to_utc_timestamp(from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;yyyy-MM-dd HH:00:00&apos;), &apos;GMT&apos;), &apos;UTC&apos;)), &apos;yyyy-MM-dd&apos;) AS gmt_date, 
from_unixtime(unix_timestamp(from_utc_timestamp(to_utc_timestamp(from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;yyyy-MM-dd HH:00:00&apos;), &apos;GMT&apos;), &apos;UTC&apos;)), &apos;HH:mm:ss&apos;) AS gmt_time,
from_unixtime(unix_timestamp(from_utc_timestamp(to_utc_timestamp(from_unixtime(unix_timestamp(`date`, &quot;yyyy-MM-dd HH:mm:ss.S&quot;), &apos;yyyy-MM-dd HH:00:00&apos;), &apos;GMT&apos;), &apos;America&#47;Los_Angeles&apos;)), &apos;yyyy-MM-dd&apos;) AS response_date_pacific, 
-100,
-100,
-100,
-100,
-100,
-100,
b.partner_sk,
b.portal_sk,
split(name,&apos;_&apos;)[0] AS partner_keyword,
split(name,&apos;_&apos;)[1] AS portal_keyword
FROM daily_millenial_log a
LEFT OUTER JOIN
(
   SELECT aa.current_millennial_media_name, bb.partner_sk, cc.portal_sk
   FROM mapping_mm aa
   INNER JOIN partner_dim bb ON bb.partner_id = aa.partner_id AND to_date(bb.dt_expire)&gt;=to_date(&apos;9999-12-31&apos;)
   INNER JOIN portal_dim cc ON cc.portal_id = aa.portal_id AND to_date(cc.dt_expire)&gt;=to_date(&apos;9999-12-31&apos;)
)b ON (a.name=b.current_millennial_media_name);
</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>Hadoop ecep</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>837</xloc>
      <yloc>49</yloc>
      </entry>
    <entry>
      <name>Hadoop Copy Files</name>
      <description/>
      <type>HadoopCopyFilesPlugin</type>
      <copy_empty_folders>N</copy_empty_folders>
      <arg_from_previous>N</arg_from_previous>
      <overwrite_files>Y</overwrite_files>
      <include_subfolders>N</include_subfolders>
      <remove_source_files>N</remove_source_files>
      <add_result_filesname>N</add_result_filesname>
      <destination_is_a_file>N</destination_is_a_file>
      <create_destination_folder>Y</create_destination_folder>
      <fields>
        <field>
          <source_filefolder>file:&#47;&#47;&#47;${local_root_dir_content_logs}&#47;</source_filefolder>
          <destination_filefolder>hdfs:&#47;&#47;ecep1&#47;user&#47;hive&#47;warehouse&#47;logs&#47;daily_millenial_log_${report_start_date}-${report_end_date}&#47;</destination_filefolder>
          <wildcard>${log_file_partent}</wildcard>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>393</xloc>
      <yloc>54</yloc>
      </entry>
    <entry>
      <name>Success</name>
      <description/>
      <type>SUCCESS</type>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1091</xloc>
      <yloc>60</yloc>
      </entry>
    <entry>
      <name>SQL Update file Proccessing</name>
      <description/>
      <type>SQL</type>
      <sql>UPDATE control.data_file
SET file_status=&apos;PS&apos;
WHERE file_name LIKE &apos;${log_file_sql_partent}&apos;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>Postgres</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>235</xloc>
      <yloc>174</yloc>
      </entry>
  </entries>
  <hops>
    <hop>
      <from>Create Working Tables</from>
      <to>Insert log data to staging</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Insert log data to staging</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Hadoop Copy Files</from>
      <to>Create Working Tables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>START</from>
      <to>SQL Update file Proccessing</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>SQL Update file Proccessing</from>
      <to>Hadoop Copy Files</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
  </notepads>
</job>
